{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"style_transfer.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"I5oZIaCsHyva","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kCa9SG7XISIY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EDn0ieuyIVuK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import sys\n","sys.path.insert(0,'drive/20180424_SK_Lab2/style_transfer')\n","\n","# Copyright (c) 2015-2017 Anish Athalye. Released under GPLv3.\n","\n","import os\n","from sys import stderr\n","\n","import tensorflow as tf\n","import numpy as np\n","import scipy.misc\n","\n","import math\n","from argparse import ArgumentParser\n","\n","from PIL import Image\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (15, 9)\n","plt.rcParams['axes.grid'] = False\n","\n","import vgg"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5pZok1uSIgOr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# default arguments\n","CONTENT_WEIGHT = 5e0\n","CONTENT_WEIGHT_BLEND = 1\n","STYLE_WEIGHT = 5e2\n","TV_WEIGHT = 1e2\n","STYLE_LAYER_WEIGHT_EXP = 1\n","LEARNING_RATE = 1e1\n","BETA1 = 0.9\n","BETA2 = 0.999\n","EPSILON = 1e-08\n","STYLE_SCALE = 1.0\n","ITERATIONS = 300\n","VGG_PATH = 'drive/20180424_SK_Lab2/style_transfer/imagenet-vgg-verydeep-19.mat'\n","POOLING = 'max'\n","\n","CONTENT_LAYERS = ('relu4_2', 'relu5_2')\n","STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wrg1YRDJYLh5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["try:\n","    reduce\n","except NameError:\n","    from functools import reduce\n","\n","\n","def stylize(network, initial, initial_noiseblend, content, styles, preserve_colors, iterations,\n","        content_weight, content_weight_blend, style_weight, style_layer_weight_exp, style_blend_weights, tv_weight,\n","        learning_rate, beta1, beta2, epsilon, pooling,\n","        print_iterations=None, checkpoint_iterations=None):\n","    \"\"\"\n","    Stylize images.\n","\n","    This function yields tuples (iteration, image); `iteration` is None\n","    if this is the final image (the last iteration).  Other tuples are yielded\n","    every `checkpoint_iterations` iterations.\n","\n","    :rtype: iterator[tuple[int|None,image]]\n","    \"\"\"\n","    shape = (1,) + content.shape\n","    style_shapes = [(1,) + style.shape for style in styles]\n","    content_features = {}\n","    style_features = [{} for _ in styles]\n","\n","    vgg_weights, vgg_mean_pixel = vgg.load_net(network)\n","\n","    layer_weight = 1.0\n","    style_layers_weights = {}\n","    for style_layer in STYLE_LAYERS:\n","        style_layers_weights[style_layer] = layer_weight\n","        layer_weight *= style_layer_weight_exp\n","\n","    # normalize style layer weights\n","    layer_weights_sum = 0\n","    for style_layer in STYLE_LAYERS:\n","        layer_weights_sum += style_layers_weights[style_layer]\n","    for style_layer in STYLE_LAYERS:\n","        style_layers_weights[style_layer] /= layer_weights_sum\n","\n","    # compute content features in feedforward mode\n","    g = tf.Graph()\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth=True\n","    config.gpu_options.visible_device_list = '0'\n","    with g.as_default(), g.device('/cpu:0'), tf.Session(config=config) as sess:\n","        image = tf.placeholder('float', shape=shape)\n","        net = vgg.net_preloaded(vgg_weights, image, pooling)\n","        content_pre = np.array([vgg.preprocess(content, vgg_mean_pixel)])\n","        for layer in CONTENT_LAYERS:\n","            content_features[layer] = net[layer].eval(feed_dict={image: content_pre})\n","\n","    # compute style features in feedforward mode\n","    for i in range(len(styles)):\n","        g = tf.Graph()\n","        with g.as_default(), g.device('/cpu:0'), tf.Session(config=config) as sess:\n","            image = tf.placeholder('float', shape=style_shapes[i])\n","            net = vgg.net_preloaded(vgg_weights, image, pooling)\n","            style_pre = np.array([vgg.preprocess(styles[i], vgg_mean_pixel)])\n","            for layer in STYLE_LAYERS:\n","                features = net[layer].eval(feed_dict={image: style_pre})\n","                features = np.reshape(features, (-1, features.shape[3]))\n","                gram = np.matmul(features.T, features) / features.size\n","                style_features[i][layer] = gram\n","\n","    initial_content_noise_coeff = 1.0 - initial_noiseblend\n","\n","    # make stylized image using backpropogation\n","    with tf.Graph().as_default():\n","        if initial is None:\n","            noise = np.random.normal(size=shape, scale=np.std(content) * 0.1)\n","            initial = tf.random_normal(shape) * 0.256\n","        else:\n","            initial = np.array([vgg.preprocess(initial, vgg_mean_pixel)])\n","            initial = initial.astype('float32')\n","            noise = np.random.normal(size=shape, scale=np.std(content) * 0.1)\n","            initial = (initial) * initial_content_noise_coeff + (tf.random_normal(shape) * 0.256) * (1.0 - initial_content_noise_coeff)\n","        image = tf.Variable(initial)\n","        net = vgg.net_preloaded(vgg_weights, image, pooling)\n","\n","        # content loss\n","        content_layers_weights = {}\n","        content_layers_weights['relu4_2'] = content_weight_blend\n","        content_layers_weights['relu5_2'] = 1.0 - content_weight_blend\n","\n","        content_loss = 0\n","        content_losses = []\n","        for content_layer in CONTENT_LAYERS:\n","            '''\n","            Compute the content loss\n","            \n","            Variables:\n","            content_weight: scalar constant we multiply the content_loss by.\n","            net[content_layer]: features of the current image, Tensor with shape [1, height, width, channels]\n","            content_features[content_layer]: features of the content image, Tensor with shape [1, height, width, channels]\n","            '''\n","            l_content = \n","            \n","            content_losses.append(content_layers_weights[content_layer] * l_content)\n","        content_loss += reduce(tf.add, content_losses)\n","\n","        # style loss\n","        style_loss = 0\n","        for i in range(len(styles)):\n","            style_losses = []\n","            for style_layer in STYLE_LAYERS:\n","                layer = net[style_layer]\n","                _, height, width, channels = map(lambda i: i.value, layer.get_shape())\n","                size = height * width * channels\n","\n","    \t        '''\n","    \t        Compute the Gram matrix of the layer\n","                    \t        \n","    \t        Variables:\n","                layer: features of the current image at style_layer, Tensor with shape [1, height, width, channels]\n","                gram: computed gram matrix with shape [channels, channels]\n","    \t        '''\n","\n","                gram = \n","                gram /= size\n","\n","    \t        '''\n","    \t        Compute the style loss\n","    \t        \n","    \t        Variables:\n","    \t        style_layers_weights[style_layer]: scalar constant we multiply the content_loss by.\n","                gram: computed gram matrix with shape [channels, channels]\n","    \t        style_gram: computed gram matrix of the style image at style_layer with shape [channels, channels]\n","    \t        '''\n","                style_gram = style_features[i][style_layer]\n","                l_style = \n","\n","                style_losses.append(l_style)\n","            style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n","\n","        # total variation denoising\n","        '''\n","        Compute the TV loss\n","            \t        \n","        Variables:\n","        tv_weight: scalar giving the weight to use for the TV loss.\n","        image: tensor of shape (1, H, W, 3) holding current image.\n","        '''\n","        tv_loss = \n","\n","        # overall loss\n","        loss = content_loss + style_loss + tv_loss\n","\n","        # optimizer setup\n","        train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)\n","\n","        def print_progress():\n","            stderr.write('  content loss: %g\\n' % content_loss.eval())\n","            stderr.write('    style loss: %g\\n' % style_loss.eval())\n","            stderr.write('       tv loss: %g\\n' % tv_loss.eval())\n","            stderr.write('    total loss: %g\\n' % loss.eval())\n","\n","        # optimization\n","        best_loss = float('inf')\n","        best = None\n","        with tf.Session(config=config) as sess:\n","            sess.run(tf.global_variables_initializer())\n","            stderr.write('Optimization started...\\n')\n","            if (print_iterations and print_iterations != 0):\n","                print_progress()\n","            for i in range(iterations):\n","                stderr.write('Iteration %4d/%4d\\n' % (i + 1, iterations))\n","                train_step.run()\n","\n","                last_step = (i == iterations - 1)\n","                if last_step or (print_iterations and i % print_iterations == 0):\n","                    print_progress()\n","\n","                if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step:\n","                    this_loss = loss.eval()\n","                    if this_loss < best_loss:\n","                        best_loss = this_loss\n","                        best = image.eval()\n","\n","                    img_out = vgg.unprocess(best.reshape(shape[1:]), vgg_mean_pixel)\n","\n","                    if preserve_colors and preserve_colors == True:\n","                        original_image = np.clip(content, 0, 255)\n","                        styled_image = np.clip(img_out, 0, 255)\n","\n","                        # Luminosity transfer steps:\n","                        # 1. Convert stylized RGB->grayscale accoriding to Rec.601 luma (0.299, 0.587, 0.114)\n","                        # 2. Convert stylized grayscale into YUV (YCbCr)\n","                        # 3. Convert original image into YUV (YCbCr)\n","                        # 4. Recombine (stylizedYUV.Y, originalYUV.U, originalYUV.V)\n","                        # 5. Convert recombined image from YUV back to RGB\n","\n","                        # 1\n","                        styled_grayscale = rgb2gray(styled_image)\n","                        styled_grayscale_rgb = gray2rgb(styled_grayscale)\n","\n","                        # 2\n","                        styled_grayscale_yuv = np.array(Image.fromarray(styled_grayscale_rgb.astype(np.uint8)).convert('YCbCr'))\n","\n","                        # 3\n","                        original_yuv = np.array(Image.fromarray(original_image.astype(np.uint8)).convert('YCbCr'))\n","\n","                        # 4\n","                        w, h, _ = original_image.shape\n","                        combined_yuv = np.empty((w, h, 3), dtype=np.uint8)\n","                        combined_yuv[..., 0] = styled_grayscale_yuv[..., 0]\n","                        combined_yuv[..., 1] = original_yuv[..., 1]\n","                        combined_yuv[..., 2] = original_yuv[..., 2]\n","\n","                        # 5\n","                        img_out = np.array(Image.fromarray(combined_yuv, 'YCbCr').convert('RGB'))\n","\n","\n","                    yield (\n","                        (None if last_step else i),\n","                        img_out\n","                    )\n","\n","\n","def _tensor_size(tensor):\n","    from operator import mul\n","    return reduce(mul, (d.value for d in tensor.get_shape()), 1)\n","\n","def rgb2gray(rgb):\n","    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n","\n","def gray2rgb(gray):\n","    w, h = gray.shape\n","    rgb = np.empty((w, h, 3), dtype=np.float32)\n","    rgb[:, :, 2] = rgb[:, :, 1] = rgb[:, :, 0] = gray\n","    return rgb"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bxkjVm10YOf-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def build_parser():\n","    parser = ArgumentParser()\n","    parser.add_argument('--content',\n","            dest='content', help='content image',\n","            metavar='CONTENT', required=True)\n","    parser.add_argument('--styles',\n","            dest='styles',\n","            nargs='+', help='one or more style images',\n","            metavar='STYLE', required=True)\n","    parser.add_argument('--iterations', type=int,\n","            dest='iterations', help='iterations (default %(default)s)',\n","            metavar='ITERATIONS', default=ITERATIONS)\n","    parser.add_argument('--print-iterations', type=int,\n","            dest='print_iterations', help='statistics printing frequency',\n","            metavar='PRINT_ITERATIONS')\n","    parser.add_argument('--checkpoint-iterations', type=int,\n","            dest='checkpoint_iterations', help='checkpoint frequency',\n","            metavar='CHECKPOINT_ITERATIONS')\n","    parser.add_argument('--width', type=int,\n","            dest='width', help='output width',\n","            metavar='WIDTH')\n","    parser.add_argument('--style-scales', type=float,\n","            dest='style_scales',\n","            nargs='+', help='one or more style scales',\n","            metavar='STYLE_SCALE')\n","    parser.add_argument('--network',\n","            dest='network', help='path to network parameters (default %(default)s)',\n","            metavar='VGG_PATH', default=VGG_PATH)\n","    parser.add_argument('--content-weight-blend', type=float,\n","            dest='content_weight_blend', help='content weight blend, conv4_2 * blend + conv5_2 * (1-blend) (default %(default)s)',\n","            metavar='CONTENT_WEIGHT_BLEND', default=CONTENT_WEIGHT_BLEND)\n","    parser.add_argument('--content-weight', type=float,\n","            dest='content_weight', help='content weight (default %(default)s)',\n","            metavar='CONTENT_WEIGHT', default=CONTENT_WEIGHT)\n","    parser.add_argument('--style-weight', type=float,\n","            dest='style_weight', help='style weight (default %(default)s)',\n","            metavar='STYLE_WEIGHT', default=STYLE_WEIGHT)\n","    parser.add_argument('--style-layer-weight-exp', type=float,\n","            dest='style_layer_weight_exp', help='style layer weight exponentional increase - weight(layer<n+1>) = weight_exp*weight(layer<n>) (default %(default)s)',\n","            metavar='STYLE_LAYER_WEIGHT_EXP', default=STYLE_LAYER_WEIGHT_EXP)\n","    parser.add_argument('--style-blend-weights', type=float,\n","            dest='style_blend_weights', help='style blending weights',\n","            nargs='+', metavar='STYLE_BLEND_WEIGHT')\n","    parser.add_argument('--tv-weight', type=float,\n","            dest='tv_weight', help='total variation regularization weight (default %(default)s)',\n","            metavar='TV_WEIGHT', default=TV_WEIGHT)\n","    parser.add_argument('--learning-rate', type=float,\n","            dest='learning_rate', help='learning rate (default %(default)s)',\n","            metavar='LEARNING_RATE', default=LEARNING_RATE)\n","    parser.add_argument('--beta1', type=float,\n","            dest='beta1', help='Adam: beta1 parameter (default %(default)s)',\n","            metavar='BETA1', default=BETA1)\n","    parser.add_argument('--beta2', type=float,\n","            dest='beta2', help='Adam: beta2 parameter (default %(default)s)',\n","            metavar='BETA2', default=BETA2)\n","    parser.add_argument('--eps', type=float,\n","            dest='epsilon', help='Adam: epsilon parameter (default %(default)s)',\n","            metavar='EPSILON', default=EPSILON)\n","    parser.add_argument('--initial',\n","            dest='initial', help='initial image',\n","            metavar='INITIAL')\n","    parser.add_argument('--initial-noiseblend', type=float,\n","            dest='initial_noiseblend', help='ratio of blending initial image with normalized noise (if no initial image specified, content image is used) (default %(default)s)',\n","            metavar='INITIAL_NOISEBLEND')\n","    parser.add_argument('--preserve-colors', action='store_true',\n","            dest='preserve_colors', help='style-only transfer (preserving colors) - if color transfer is not needed')\n","    parser.add_argument('--pooling',\n","            dest='pooling', help='pooling layer configuration: max or avg (default %(default)s)',\n","            metavar='POOLING', default=POOLING)\n","    return parser\n","  \n","\n","def style_transfer(content, style, args=None):\n","    parser = build_parser()\n","    args_list = ['--content', content, '--styles', style]\n","    if args is not None:\n","      args_list += [args]\n","    options = parser.parse_args(args_list)\n","\n","    if not os.path.isfile(options.network):\n","        parser.error(\"Network %s does not exist. (Did you forget to download it?)\" % options.network)\n","\n","    content_image = imread(options.content)\n","    style_images = [imread(style) for style in options.styles]\n","\n","    width = options.width\n","    if width is not None:\n","        new_shape = (int(math.floor(float(content_image.shape[0]) /\n","                content_image.shape[1] * width)), width)\n","        content_image = scipy.misc.imresize(content_image, new_shape)\n","    target_shape = content_image.shape\n","    for i in range(len(style_images)):\n","        style_scale = STYLE_SCALE\n","        if options.style_scales is not None:\n","            style_scale = options.style_scales[i]\n","        style_images[i] = scipy.misc.imresize(style_images[i], style_scale *\n","                target_shape[1] / style_images[i].shape[1])\n","\n","    style_blend_weights = options.style_blend_weights\n","    if style_blend_weights is None:\n","        # default is equal weights\n","        style_blend_weights = [1.0/len(style_images) for _ in style_images]\n","    else:\n","        total_blend_weight = sum(style_blend_weights)\n","        style_blend_weights = [weight/total_blend_weight\n","                               for weight in style_blend_weights]\n","\n","    initial = options.initial\n","    if initial is not None:\n","        initial = scipy.misc.imresize(imread(initial), content_image.shape[:2])\n","        # Initial guess is specified, but not noiseblend - no noise should be blended\n","        if options.initial_noiseblend is None:\n","            options.initial_noiseblend = 0.0\n","    else:\n","        # Neither inital, nor noiseblend is provided, falling back to random generated initial guess\n","        if options.initial_noiseblend is None:\n","            options.initial_noiseblend = 1.0\n","        if options.initial_noiseblend < 1.0:\n","            initial = content_image\n","\n","    for iteration, image in stylize(\n","        network=options.network,\n","        initial=initial,\n","        initial_noiseblend=options.initial_noiseblend,\n","        content=content_image,\n","        styles=style_images,\n","        preserve_colors=options.preserve_colors,\n","        iterations=options.iterations,\n","        content_weight=options.content_weight,\n","        content_weight_blend=options.content_weight_blend,\n","        style_weight=options.style_weight,\n","        style_layer_weight_exp=options.style_layer_weight_exp,\n","        style_blend_weights=style_blend_weights,\n","        tv_weight=options.tv_weight,\n","        learning_rate=options.learning_rate,\n","        beta1=options.beta1,\n","        beta2=options.beta2,\n","        epsilon=options.epsilon,\n","        pooling=options.pooling,\n","        print_iterations=options.print_iterations,\n","        checkpoint_iterations=options.checkpoint_iterations\n","    ):\n","        combined_rgb = image\n","        \n","    plt.figure()\n","    plt.imshow(content_image / 255)\n","    plt.xlabel('CONTENT IMAGE')\n","    for i in range(len(style_images)):\n","      plt.figure()\n","      plt.imshow(style_images[i] / 255)\n","      plt.xlabel('STYLE IMAGE {}'.format(i))\n","    plt.figure()\n","    plt.imshow(np.clip(combined_rgb, 0, 255) / 255)\n","    plt.xlabel('OUTPUT')\n","\n","\n","def imread(path):\n","    img = scipy.misc.imread(path).astype(np.float)\n","    if len(img.shape) == 2:\n","        # grayscale\n","        img = np.dstack((img,img,img))\n","    elif img.shape[2] == 4:\n","        # PNG with alpha channel\n","        img = img[:,:,:3]\n","    return img"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YVI_BxYLYX_O","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["style_transfer('drive/20180424_SK_Lab2/style_transfer/examples/1-content.jpg',\n","               'drive/20180424_SK_Lab2/style_transfer/examples/1-style.jpg')"],"execution_count":0,"outputs":[]}]}