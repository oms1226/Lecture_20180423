{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"super_resolution.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"5TyqPhmCvHf9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RtAqOOGjvLSw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b2RKc6HRvNNt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["ROOT = 'drive/20180424_SK_Lab2/super_resolution'\n","\n","import sys\n","sys.path.insert(0, ROOT)\n","\n","import tensorflow as tf\n","import numpy as np\n","import time\n","import glob\n","from PIL import Image\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (15, 9)\n","plt.rcParams['axes.grid'] = False\n","\n","from layers import *"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i0M0Tm1FvkRr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Compute PSNR\n","def PSNR(y_true, y_pred, shave_border=4, maxVal=255):      \n","    target_data = np.array(y_true, dtype=np.float32)\n","    ref_data = np.array(y_pred, dtype=np.float32)\n","    diff = ref_data - target_data\n","    if shave_border > 0:\n","        diff = diff[shave_border:-shave_border, shave_border:-shave_border]\n","    rmse = np.sqrt(np.mean(np.power(diff, 2)))\n","    return 20 * np.log10(maxVal/rmse)\n","\n","# RGB2YCbCr\n","def _rgb2ycbcr(img, maxVal=255):\n","    O = np.array([[16],\n","                  [128],\n","                  [128]])\n","    T = np.array([[0.256788235294118, 0.504129411764706, 0.097905882352941],\n","                  [-0.148223529411765, -0.290992156862745, 0.439215686274510],\n","                  [0.439215686274510, -0.367788235294118, -0.071427450980392]])\n","    if maxVal == 1:\n","        O = O / 255.0\n","    t = np.reshape(img, (img.shape[0]*img.shape[1], img.shape[2]))\n","    t = np.dot(t, np.transpose(T))\n","    t[:, 0] += O[0]\n","    t[:, 1] += O[1]\n","    t[:, 2] += O[2]\n","    ycbcr = np.reshape(t, [img.shape[0], img.shape[1], img.shape[2]])\n","    return ycbcr"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q3rnDyExbLiZ","colab_type":"text"},"cell_type":"markdown","source":["# SRCNN"]},{"metadata":{"id":"WRMYvRvivsTx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Network parameters\n","B = 4\n","H = 32\n","W = 32\n","C = 1\n","r = 2    # scale factor for SR\n","lr_init = 0.0001\n","momentum = 0.9\n","\n","tf.reset_default_graph()\n","\n","# Network\n","def SRCNN(x):\n","    '''\n","    Your code here\n","    '''\n","\n","    return x\n","\n","# Whole model\n","inputs = tf.placeholder(tf.float32, shape=[None, None, None, 1])\n","labels = tf.placeholder(tf.float32, shape=[None, None, None, 1])\n","lr = tf.placeholder(tf.float32, shape=[])\n","\n","with tf.variable_scope('SRCNN') as scope:\n","    outputs = SRCNN(inputs)\n","    \n","# Mean squared error\n","loss = \n","    '''\n","    Your code here\n","    '''\n","\n","def do_SRCNN(phase):\n","    assert phase == 'Train' or phase == 'Test'\n","\n","    # Momentum optimizer\n","    if phase == 'Train':\n","        trainable_vars_last_layer = [v for v in tf.trainable_variables() if 'conv3' in v.name]\n","        trainable_vars = [v for v in tf.trainable_variables() if v.name.startswith('SRCNN/') and not v in trainable_vars_last_layer]\n","\n","        opt1 = tf.train.MomentumOptimizer(lr, momentum)\n","        opt2 = tf.train.MomentumOptimizer(lr*0.1, momentum)\n","\n","        grads = tf.gradients(loss, trainable_vars + trainable_vars_last_layer)\n","        grads1 = grads[:len(trainable_vars)]\n","        grads2 = grads[len(trainable_vars):]\n","\n","        train_op1 = opt1.apply_gradients(zip(grads1, trainable_vars))\n","        train_op2 = opt2.apply_gradients(zip(grads2, trainable_vars_last_layer))\n","        train_SRCNN = tf.group(train_op1, train_op2)\n","\n","    # Data preparation\n","    def PrepareTrainImages(train_data):\n","        # Training images\n","        train_labels = np.zeros((len(train_data), H - 12, W - 12, C), dtype=np.uint8)\n","        train_images = np.zeros((len(train_data), H, W, C), dtype=np.uint8)\n","        for i in range(len(train_data)):\n","            img = Image.fromarray(train_data[i])\n","            # Bicubic down-upsampling\n","            w, h = img.size\n","            img_input = img.resize((w//r, h//r), Image.ANTIALIAS)\n","            img_input = img_input.resize((w, h), Image.BICUBIC)\n","            img = np.asarray(img)\n","            img_input = np.asarray(img_input)\n","            # Random crop\n","            r_y = np.random.randint(img.shape[0] - H)\n","            r_x = np.random.randint(img.shape[1] - W)\n","            img = img[r_y:r_y+H, r_x:r_x+W, :]\n","            img_input = img_input[r_y:r_y+H, r_x:r_x+W, :]\n","            # Random LR flip\n","            if np.random.random() < 0.5:\n","                img = np.copy(img[:, ::-1, :])\n","                img_input = np.copy(img_input[:, ::-1, :])\n","            train_labels[i, :, :, :] = _rgb2ycbcr(img)[6:-6, 6:-6, 0:1]\n","            train_images[i, :, :, :] = _rgb2ycbcr(img_input)[:, :, 0:1]\n","        train_labels = train_labels / 255.0\n","        train_images = train_images / 255.0\n","        num_training_samples = train_images.shape[0]\n","\n","        return train_images, train_labels, num_training_samples\n","\n","    train_data = np.load(ROOT + '/train_91.npy')\n","    train_images, train_labels, num_training_samples = PrepareTrainImages(train_data)\n","\n","    # Test images\n","    test_filenames = glob.glob(ROOT + '/Set5/*.bmp')\n","    num_test_samples = len(test_filenames)\n","\n","    # TF saver\n","    saver = tf.train.Saver()\n","\n","    # TF Session\n","    with tf.Session() as sess:\n","        # Load or init variables\n","        if phase == 'Train':\n","            tf.global_variables_initializer().run()\n","        else:\n","            saver.restore(sess, ROOT + '/SRCNN_models/SRCNN_iter10000.ckpt')\n","            print(\"Model restored.\")\n","\n","        if phase == 'Train':\n","            e = 0 # epoch\n","            p = 0 # pointer\n","            lr_curr = lr_init  # learning rate\n","\n","            # Training\n","            for i in range(0, 200000):\n","                t = time.time()\n","                l_total,  _= sess.run([loss, train_SRCNN], feed_dict={inputs: train_images[p:p+B], labels: train_labels[p:p+B], lr:lr_curr})\n","                dT = time.time() - t\n","                if (i + 1) % 100 == 0:\n","                    print('Epoch: {:3d} | Iter: {:4d} | Loss: {:4.3e} | dT: {:4.3f}s'.format(e + 1, i + 1, l_total, dT))\n","\n","                p += B\n","                if p >= num_training_samples:\n","                    e += 1\n","                    p = 0\n","                    train_images, train_labels, num_training_samples = PrepareTrainImages(train_data)\n","\n","                if (i + 1) % 10000 == 0:\n","                    save_path = saver.save(sess, ROOT + '/SRCNN_models/SRCNN_iter'+str(i + 1)+'.ckpt')\n","                    print(\"Model saved in file: %s\" % save_path)\n","        elif phase == 'Test':\n","          # Test\n","          for i in range(0, num_test_samples):\n","              # Read a test image\n","              img = Image.open(test_filenames[i])\n","              img = img.convert('RGB')\n","              # Bicubic down-upsampling\n","              w, h = img.size\n","              img_input = img.resize((w//r, h//r), Image.ANTIALIAS)\n","              img_input = img_input.resize((w, h), Image.BICUBIC)\n","              img = np.asarray(img)\n","              img = _rgb2ycbcr(img)[:, :, 0:1]\n","              img_input = np.asarray(img_input)\n","              img_input = _rgb2ycbcr(img_input)[:, :, 0:1]\n","              img_input = img_input / 255.0\n","\n","              t = time.time()\n","              output = sess.run(outputs, feed_dict={inputs: img_input[np.newaxis, ]})\n","              dT = time.time() - t\n","              res = (np.clip(output[0,:,:,0],0,1)*255).astype(np.uint8)\n","\n","              fig = plt.figure()\n","              ax1 = fig.add_subplot(1, 3, 1)\n","              ax1.imshow(img_input[6:-6,6:-6,0], cmap='gray')\n","              ax1.set_xlabel('BICUBIC')\n","              ax2 = fig.add_subplot(1, 3, 2)\n","              ax2.imshow(np.clip(output[0,:,:,0],0,1), cmap='gray')\n","              ax2.set_xlabel('SRCNN')\n","              ax3 = fig.add_subplot(1, 3, 3)\n","              ax3.imshow(img[6:-6, 6:-6, 0] / 255.0, cmap='gray')\n","              ax3.set_xlabel('GROUND TRUTH')\n","              plt.title('Test image #: {:3d} | PSNR: {:3f} | dT: {:4.3f}s'.format(i, PSNR(img[6:-6, 6:-6, 0], res), dT))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yUdHnM_g6jSC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["do_SRCNN('Train')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kD7n2jgi8mcf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["do_SRCNN('Test')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Vx0_H_YGbedm","colab_type":"text"},"cell_type":"markdown","source":["# VDSR"]},{"metadata":{"id":"Idf32x7Bbgcz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Network parameters\n","B = 4\n","H = 41\n","W = 41\n","C = 1\n","r = 2    # scale factor for SR\n","\n","tf.reset_default_graph()\n","\n","# Network\n","def VDSR(x):\n","    '''\n","    Your code here\n","    '''   \n","    return x\n","\n","# Whole model\n","inputs = tf.placeholder(tf.float32, shape=[None, None, None, 1])\n","labels = tf.placeholder(tf.float32, shape=[None, None, None, 1])\n","\n","with tf.variable_scope('VDSR') as scope:\n","    outputs = VDSR(inputs)\n","    \n","# Mean squared error\n","loss = tf.reduce_mean(tf.square(outputs - labels))\n","\n","def do_VDSR(phase):\n","    assert phase == 'Train' or phase == 'Test'\n","\n","    # Momentum optimizer\n","    if phase == 'Train':\n","        trainable_vars = [v for v in tf.trainable_variables() if v.name.startswith('VDSR/')]\n","        opt = tf.train.AdamOptimizer(0.001)\n","        grads_and_vars = opt.compute_gradients(loss, var_list=trainable_vars)\n","        grads_and_vars = [(tf.clip_by_norm(x[0], 0.1), x[1]) for x in grads_and_vars]\n","        train_VDSR = opt.apply_gradients(grads_and_vars)\n","\n","    # Data preparation\n","    def PrepareTrainImages(train_data):\n","        # Training images\n","        train_labels = np.zeros((len(train_data), H, W, C), dtype=np.uint8)\n","        train_images = np.zeros((len(train_data), H, W, C), dtype=np.uint8)\n","        for i in range(len(train_data)):\n","            img = Image.fromarray(train_data[i])\n","            # Bicubic down-upsampling\n","            w, h = img.size\n","            img_input = img.resize((w//r, h//r), Image.ANTIALIAS)\n","            img_input = img_input.resize((w, h), Image.BICUBIC)\n","            img = np.asarray(img)\n","            img_input = np.asarray(img_input)\n","            # Random crop\n","            r_y = np.random.randint(img.shape[0] - H)\n","            r_x = np.random.randint(img.shape[1] - W)\n","            img = img[r_y:r_y+H, r_x:r_x+W, :]\n","            img_input = img_input[r_y:r_y+H, r_x:r_x+W, :]\n","            # Random LR flip\n","            if np.random.random() < 0.5:\n","                img = np.copy(img[:, ::-1, :])\n","                img_input = np.copy(img_input[:, ::-1, :])\n","            train_labels[i, :, :, :] = _rgb2ycbcr(img)[:, :, 0:1]\n","            train_images[i, :, :, :] = _rgb2ycbcr(img_input)[:, :, 0:1]\n","        train_labels = train_labels / 255.0\n","        train_images = train_images / 255.0\n","        num_training_samples = train_images.shape[0]\n","\n","        return train_images, train_labels, num_training_samples\n","\n","    train_data = np.load(ROOT + '/train_91.npy')\n","    train_images, train_labels, num_training_samples = PrepareTrainImages(train_data)\n","\n","    # Test images\n","    test_filenames = glob.glob(ROOT + '/Set5/*.bmp')\n","    num_test_samples = len(test_filenames)\n","\n","    # TF saver\n","    saver = tf.train.Saver()\n","\n","    # TF Session\n","    with tf.Session() as sess:\n","        # Load or init variables\n","        if phase == 'Train':\n","            tf.global_variables_initializer().run()\n","        else:\n","            saver.restore(sess, ROOT + '/VDSR_models/VDSR_iter10000.ckpt')\n","            print(\"Model restored.\")\n","\n","        if phase == 'Train':\n","            e = 0 # epoch\n","            p = 0 # pointer\n","\n","            # Training\n","            for i in range(0, 50000):\n","                t = time.time()\n","                l_total, _ = sess.run([loss, train_VDSR], feed_dict={inputs: train_images[p:p+B], labels: train_labels[p:p+B]})\n","                dT = time.time() - t\n","                if (i + 1) % 100 == 0:\n","                    print('Epoch: {:3d} | Iter: {:4d} | Loss: {:4.3e} | dT: {:4.3f}s'.format(e + 1, i + 1, l_total, dT))\n","\n","                p += B\n","                if p >= num_training_samples:\n","                    e += 1\n","                    p = 0\n","                    train_images, train_labels, num_training_samples = PrepareTrainImages(train_data)\n","\n","                if (i + 1) % 10000 == 0:\n","                    save_path = saver.save(sess, ROOT + '/VDSR_models/VDSR_iter'+str(i + 1)+'.ckpt')\n","                    print(\"Model saved in file: %s\" % save_path)\n","        elif phase == 'Test':\n","          # Test\n","          for i in range(0, num_test_samples):\n","              # Read a test image\n","              img = Image.open(test_filenames[i])\n","              img = img.convert('RGB')\n","              # Bicubic down-upsampling\n","              w, h = img.size\n","              img_input = img.resize((w//r, h//r), Image.ANTIALIAS)\n","              img_input = img_input.resize((w, h), Image.BICUBIC)\n","              img = np.asarray(img)\n","              img = _rgb2ycbcr(img)[:, :, 0:1]\n","              img_input = np.asarray(img_input)\n","              img_input = _rgb2ycbcr(img_input)[:, :, 0:1]\n","              img_input = img_input / 255.0\n","\n","              t = time.time()\n","              output = sess.run(outputs, feed_dict={inputs: img_input[np.newaxis, ]})\n","              dT = time.time() - t\n","              res = (np.clip(output[0,:,:,0],0,1)*255).astype(np.uint8)\n","\n","              fig = plt.figure()\n","              ax1 = fig.add_subplot(1, 3, 1)\n","              ax1.imshow(img_input[:,:,0], cmap='gray')\n","              ax1.set_xlabel('BICUBIC')\n","              ax2 = fig.add_subplot(1, 3, 2)\n","              ax2.imshow(np.clip(output[0,:,:,0],0,1), cmap='gray')\n","              ax2.set_xlabel('VDSR')\n","              ax3 = fig.add_subplot(1, 3, 3)\n","              ax3.imshow(img[:, :, 0] / 255.0, cmap='gray')\n","              ax3.set_xlabel('GROUND TRUTH')\n","              plt.title('Test image #: {:3d} | PSNR: {:3f} | dT: {:4.3f}s'.format(i, PSNR(img[:, :, 0], res), dT))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3dk2cW-JbqVp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["do_VDSR('Train')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U5b5qmstbsDf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["do_VDSR('Test')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L8cgrbb9lDtE","colab_type":"text"},"cell_type":"markdown","source":["# VDSR SP"]},{"metadata":{"id":"708a4S5ElGjN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Huber error\n","def Huber(y_true, y_pred, delta, axis=None):\n","    abs_error = tf.abs(y_pred - y_true)\n","    quadratic = tf.minimum(abs_error, delta)\n","    # The following expression is the same in value as\n","    # tf.maximum(abs_error - delta, 0), but importantly the gradient for the\n","    # expression when abs_error == delta is 0 (for tf.maximum it would be 1).\n","    # This is necessary to avoid doubling the gradient, since there is already a\n","    # nonzero contribution to the gradient from the quadratic term.\n","    linear = (abs_error - quadratic)\n","    losses = 0.5 * quadratic**2 + delta * linear\n","    return tf.reduce_mean(losses, axis=axis)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MIXmUS1IlN1f","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Network parameters\n","B = 4\n","H = 24\n","W = 24\n","C = 1\n","r = 2    # scale factor for SR\n","\n","tf.reset_default_graph()\n","\n","# Network\n","def VDSR_SP(x):\n","    '''\n","    Your code here\n","    '''\n","    return x\n","\n","# Whole model\n","inputs = tf.placeholder(tf.float32, shape=[None, None, None, 1])\n","labels = tf.placeholder(tf.float32, shape=[None, None, None, 1])\n","\n","with tf.variable_scope('VDSR') as scope:\n","    outputs = VDSR_SP(inputs)\n","    \n","# Mean squared error\n","loss = Huber(labels, outputs, 0.01)\n","\n","def do_VDSR_SP(phase):\n","    assert phase == 'Train' or phase == 'Test'\n","\n","    # Momentum optimizer\n","    if phase == 'Train':\n","        trainable_vars = [v for v in tf.trainable_variables() if v.name.startswith('VDSR/')]\n","        opt = tf.train.AdamOptimizer(0.001)\n","        grads_and_vars = opt.compute_gradients(loss, var_list=trainable_vars)\n","        grads_and_vars = [(tf.clip_by_norm(x[0], 0.1), x[1]) for x in grads_and_vars]\n","        train_VDSR = opt.apply_gradients(grads_and_vars)\n","\n","    # Data preparation\n","    def PrepareTrainImages(train_data):\n","        # Training images\n","        train_labels = np.zeros((len(train_data), H*r, W*r, C), dtype=np.uint8)\n","        train_images = np.zeros((len(train_data), H, W, C), dtype=np.uint8)\n","        for i in range(len(train_data)):\n","            img = Image.fromarray(train_data[i])\n","            # Bicubic downsampling\n","            w, h = img.size\n","            img_input = img.resize((w//r, h//r), Image.ANTIALIAS)\n","            img = np.asarray(img)\n","            img_input = np.asarray(img_input)\n","            # Random crop\n","            r_y = np.random.randint(img_input.shape[0] - H)\n","            r_x = np.random.randint(img_input.shape[1] - W)\n","            img = img[r_y*r:(r_y+H)*r, r_x*r:(r_x+W)*r, :]\n","            img_input = img_input[r_y:r_y+H, r_x:r_x+W, :]\n","            # Random LR flip\n","            if np.random.random() < 0.5:\n","                img = np.copy(img[:, ::-1, :])\n","                img_input = np.copy(img_input[:, ::-1, :])\n","            train_labels[i, :, :, :] = _rgb2ycbcr(img)[:, :, 0:1]\n","            train_images[i, :, :, :] = _rgb2ycbcr(img_input)[:, :, 0:1]\n","        train_labels = train_labels / 255.0\n","        train_images = train_images / 255.0\n","        num_training_samples = train_images.shape[0]\n","\n","        return train_images, train_labels, num_training_samples\n","\n","    train_data = np.load(ROOT + '/train_91.npy')\n","    train_images, train_labels, num_training_samples = PrepareTrainImages(train_data)\n","\n","    # Test images\n","    test_filenames = glob.glob(ROOT + '/Set5/*.bmp')\n","    num_test_samples = len(test_filenames)\n","\n","    # TF saver\n","    saver = tf.train.Saver()\n","\n","    # TF Session\n","    with tf.Session() as sess:\n","        # Load or init variables\n","        if phase == 'Train':\n","            tf.global_variables_initializer().run()\n","        else:\n","            saver.restore(sess, ROOT + '/VDSR_models/VDSR_SP_iter10000.ckpt')\n","            print(\"Model restored.\")\n","\n","        if phase == 'Train':\n","            e = 0 # epoch\n","            p = 0 # pointer\n","\n","            # Training\n","            for i in range(0, 50000):\n","                t = time.time()\n","                l_total, _ = sess.run([loss, train_VDSR], feed_dict={inputs: train_images[p:p+B], labels: train_labels[p:p+B]})\n","                dT = time.time() - t\n","                if (i + 1) % 100 == 0:\n","                    print('Epoch: {:3d} | Iter: {:4d} | Loss: {:4.3e} | dT: {:4.3f}s'.format(e + 1, i + 1, l_total, dT))\n","\n","                p += B\n","                if p >= num_training_samples:\n","                    e += 1\n","                    p = 0\n","                    train_images, train_labels, num_training_samples = PrepareTrainImages(train_data)\n","\n","                if (i + 1) % 10000 == 0:\n","                    save_path = saver.save(sess, ROOT + '/VDSR_models/VDSR_SP_iter'+str(i + 1)+'.ckpt')\n","                    print(\"Model saved in file: %s\" % save_path)\n","        elif phase == 'Test':\n","          # Test\n","          for i in range(0, num_test_samples):\n","              # Read a test image\n","              img = Image.open(test_filenames[i])\n","              img = img.convert('RGB')\n","              # Bicubic down-upsampling\n","              w, h = img.size\n","              img_input = img.resize((w//r, h//r), Image.ANTIALIAS)\n","              img = np.asarray(img)\n","              img = _rgb2ycbcr(img)[:, :, 0:1]\n","              img_input = np.asarray(img_input)\n","              img_input = _rgb2ycbcr(img_input)[:, :, 0:1]\n","              img_input = img_input / 255.0\n","\n","              t = time.time()\n","              output = sess.run(outputs, feed_dict={inputs: img_input[np.newaxis, ]})\n","              dT = time.time() - t\n","              res = (np.clip(output[0,:,:,0],0,1)*255).astype(np.uint8)\n","\n","              fig = plt.figure()\n","              ax1 = fig.add_subplot(1, 3, 1)\n","              ax1.imshow(img_input[:,:,0], cmap='gray')\n","              ax1.set_xlabel('BICUBIC')\n","              ax2 = fig.add_subplot(1, 3, 2)\n","              ax2.imshow(np.clip(output[0,:,:,0],0,1), cmap='gray')\n","              ax2.set_xlabel('VDSR SP')\n","              ax3 = fig.add_subplot(1, 3, 3)\n","              ax3.imshow(img[:, :, 0] / 255.0, cmap='gray')\n","              ax3.set_xlabel('GROUND TRUTH')\n","              plt.title('Test image #: {:3d} | PSNR: {:3f} | dT: {:4.3f}s'.format(i, PSNR(img[:, :, 0], res), dT))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zj5uMkFvlycM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["do_VDSR_SP('Train')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gc77O23Dl1Bk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["do_VDSR_SP('Test')"],"execution_count":0,"outputs":[]}]}